[[
  name: getDevice
  python_name: get_device
  defined_if: IS_CUDA
  return: long
  arguments:
    - THTensor* self
]]

[[
  name: THPTensor_(new)
  python_name: new
  method_flags: METH_KEYWORDS
  defined_if: IS_CUDA
  only_register: True
]]
#if IS_CUDA
static PyObject * THPTensor_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs);
PyObject * THPTensor_(new)(THPTensor *self, PyObject *args, PyObject *kwargs)
{
  THCPAutoGPU gpu_guard(args, (PyObject*)self);
  return THPTensor_(pynew)(Py_TYPE(self), args, kwargs);
}
#endif

[[
  name: THPTensor_(recordStream)
  python_name: record_stream
  override_method_flags: METH_O
  defined_if: IS_CUDA
  only_register: True
]]
#if IS_CUDA
PyObject * THPTensor_(recordStream)(THPTensor *self, PyObject *arg)
{
  HANDLE_TH_ERRORS
  if (!THCPStream_Check(arg)) {
    return PyErr_Format(PyExc_TypeError, "expected Stream object");
  }
  void* data = THTensor_(data)(LIBRARY_STATE self->cdata);
  THCCachingAllocator_recordStream(data, ((THCPStream*)arg)->cdata);
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}
#endif
